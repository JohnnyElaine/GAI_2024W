{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2a3156-f1b7-44a4-b3d1-699c887777ba",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d896e0259d4153e4d3217f309dd2c76",
     "grade": false,
     "grade_id": "cell-ebbf630e1db53625",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2024.11.13 - Policy Objective Function\n",
    "Proximal Policy Optimization (PPO) is a policy gradient method for reinforcement learning used in the final training stage of GPT. In this homework, you will look at the clipped surrogate objective function:\n",
    "\n",
    "$L^{PG}(\\theta) = E_t[\\log \\pi_\\theta(a_t|s_t) * A_t]$\n",
    "\n",
    "We know there is quite a long path between this exercise and the complete application of PPO in RLHF. However, since this is an introductory lecture and due to the vast diversity of the audience, we will not go into more depth.\n",
    "In case you are interested in diving deeper into the realm of reinforcement learning and PPO, check the following resources:\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "- [Lilian Weng Policy Gradient Algorithms](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)\n",
    "- [Proximal Policy Optimization (PPO): The Key to LLM Alignment](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)\n",
    "\n",
    "Base your code on the following skeleton code that we provide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4a4b4-d89a-4e48-a7bd-1a2361bd7aa9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e013d57a6d20fec824120fadea4db577",
     "grade": false,
     "grade_id": "cell-b3bd1f4063046332",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### The context of this exercise\n",
    "In RLHF, the first step is that a pre-trained model generates multiple responses for individual prompts. In the second stage humans annotate those prompts with their preferences (this can be ordering them, answering yes/no questions like is this harmful or rating individual completions on numerical scales). Using these preferences, a separate reward model is trained, whose purpose is to predict human preferences and critique or judge future outputs of the language model.\n",
    "\n",
    "In the final stage, the objective function $L^{PG}(\\theta)$ is used in the final stage where the language model weights are updated to align with human preferences. It ensures that the model maximizes the reward predicted by the reward model (through advantages). Ratio clipping ensures that there are no overly extreme updates in individual steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eaeb06-0cb3-4829-9124-e4d8fda62e6e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "72b9f70f69e6eb4ed835cadea2d620e3",
     "grade": false,
     "grade_id": "cell-0edaca24c2eef535",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de9ac02-3d71-4493-830e-6cc6631685aa",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9fc014827e313cd5a24b6300a750277",
     "grade": false,
     "grade_id": "cell-1515e5e0f1e23c2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "def ppo_loss_components(old_probs, new_probs, advantages, epsilon=0.2):\n    \"\"\"Calculate the components of PPO's clipped surrogate objective function.\n    \n    This function implements the core PPO clipping mechanism that prevents excessive\n    policy updates. It returns both the final loss and intermediate values for analysis.\n    \n    Args:\n        old_probs (torch.Tensor): Probabilities from the old policy\n        new_probs (torch.Tensor): Probabilities from the new policy\n        advantages (torch.Tensor): Advantage estimates\n        epsilon (float, optional): Clipping parameter. Defaults to 0.2\n        \n    Returns:\n        dict: Dictionary containing:\n            - 'ratio': Probability ratios (π_new / π_old)\n            - 'clipped_ratio': Ratios clipped to [1-ε, 1+ε]\n            - 'surr1': First surrogate term (ratio * advantage)\n            - 'surr2': Second surrogate term (clipped_ratio * advantage)\n            - 'loss': Final PPO loss (negative because we want to maximize)\n            \n    Example Usage:\n        >>> old_p = torch.tensor([0.5, 0.8])\n        >>> new_p = torch.tensor([0.75, 0.3])\n        >>> adv = torch.tensor([0.75])\n        >>> result = ppo_loss_components(old_p, new_p, adv)\n        >>> print(f\"Loss: {result['loss']:.3f}\")\n    \"\"\"\n    # TODO: Implement the following steps:\n    # 1. Calculate probability ratio (π_new / π_old)\n    # 2. Clip the ratio to stay within [1-ε, 1+ε]\n    # 3. Calculate both surrogate terms (see doc string)\n    # 4. Take minimum of the surrogates and from that the mean for final loss.\n    # \n    # 5. (already implemented) Return all components in a dictionary\n\n    # Hints:\n    # - Should be doable in 5 lines of code | don't overcomplicate it\n    # - You might use torch.clamp(input, min, max) for clipping\n    # - Remember to make the loss negative since we want to maximize the objective\n        \n    # YOUR CODE HERE\n    raise NotImplementedError()\n\n    return {\n        'ratio': ratio,\n        'clipped_ratio': clipped_ratio,\n        'surr1': surr1,\n        'surr2': surr2,\n        'loss': loss\n    }"
   },
   "outputs": [],
   "source": [
    "def ppo_loss_components(old_probs, new_probs, advantages, epsilon=0.2):\n",
    "    \"\"\"Calculate the components of PPO's clipped surrogate objective function.\n",
    "    \n",
    "    This function implements the core PPO clipping mechanism that prevents excessive\n",
    "    policy updates. It returns both the final loss and intermediate values for analysis.\n",
    "    \n",
    "    Args:\n",
    "        old_probs (torch.Tensor): Probabilities from the old policy\n",
    "        new_probs (torch.Tensor): Probabilities from the new policy\n",
    "        advantages (torch.Tensor): Advantage estimates\n",
    "        epsilon (float, optional): Clipping parameter. Defaults to 0.2\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing:\n",
    "            - 'ratio': Probability ratios (π_new / π_old)\n",
    "            - 'clipped_ratio': Ratios clipped to [1-ε, 1+ε]\n",
    "            - 'surr1': First surrogate term (ratio * advantage)\n",
    "            - 'surr2': Second surrogate term (clipped_ratio * advantage)\n",
    "            - 'loss': Final PPO loss (negative because we want to maximize)\n",
    "            \n",
    "    Example Usage:\n",
    "        >>> old_p = torch.tensor([0.5, 0.8])\n",
    "        >>> new_p = torch.tensor([0.75, 0.3])\n",
    "        >>> adv = torch.tensor([0.75])\n",
    "        >>> result = ppo_loss_components(old_p, new_p, adv)\n",
    "        >>> print(f\"Loss: {result['loss']:.3f}\")\n",
    "    \"\"\"\n",
    "    # TODO: Implement the following steps:\n",
    "    # 1. Calculate probability ratio (π_new / π_old)\n",
    "    # 2. Clip the ratio to stay within [1-ε, 1+ε]\n",
    "    # 3. Calculate both surrogate terms (see doc string)\n",
    "    # 4. Take minimum of the surrogates and from that the mean for final loss.\n",
    "    # \n",
    "    # 5. (already implemented) Return all components in a dictionary\n",
    "\n",
    "    # Hints:\n",
    "    # - Should be doable in 5 lines of code | don't overcomplicate it\n",
    "    # - You might use torch.clamp(input, min, max) for clipping\n",
    "    # - Remember to make the loss negative since we want to maximize the objective\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "    return {\n",
    "        'ratio': ratio,\n",
    "        'clipped_ratio': clipped_ratio,\n",
    "        'surr1': surr1,\n",
    "        'surr2': surr2,\n",
    "        'loss': loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556cf89a-491b-4ba1-b346-4707b9f78ad1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46e14f3a746ca497e077c663e30690f4",
     "grade": false,
     "grade_id": "cell-9f4b3f7b463fc597",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "#### What should you see here?\n",
    "The graphs illustrate the key mechanics of PPO's clipping mechanism with positive advantages.\n",
    "The top graph shows how PPO clips the probability ratio ($\\pi_{new}/\\pi_{old}$) to stay within [0.8, 1.2] (with $\\epsilon$=0.2). The blue line shows the unclipped ratio, while the red line shows how clipping constrains it.\n",
    "The bottom graph shows three components:\n",
    "\n",
    "- Blue line (Surrogate 1): The unclipped objective (ratio * advantage)\n",
    "- Red line (Surrogate 2): The clipped objective\n",
    "- Green line (Final objective): The minimum of the two surrogates\n",
    "\n",
    "You should see that clipping prevents excessive policy updates by flattening the objective when the ratio moves too far from 1.0. This creates a pessimistic bound on the improvement and helps maintain stable training by avoiding large policy changes.\n",
    "For example, at a ratio of 1.5, the original objective would continue growing linearly, but clipping ensures the actual update remains moderate.\n",
    "The asymmetry of clipping ensures that if an action produces a good result, we don't need to make it dramatically more likely— a moderate increase is sufficient. However, if an action produces an undesired result (negative advantage), we want to allow stronger updates to reduce its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84042a3e-6c08-4d6f-8ca4-9dbbf18047ad",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ec034397d6f7850173d75fd7b97c2fd",
     "grade": false,
     "grade_id": "cell-4d73ff57b314a839",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test scenario 1: Fixed old_prob with varying new_prob\n",
    "old_prob = torch.tensor([1.0])\n",
    "new_probs = torch.tensor(np.linspace(0.5, 2.0, 100)).float()\n",
    "advantages = torch.tensor([.8])  # Positive advantage\n",
    "\n",
    "# Calculate components across range of probability ratios\n",
    "results = []\n",
    "for new_p in new_probs:\n",
    "    components = ppo_loss_components(old_prob, new_p.reshape(1), advantages)\n",
    "    results.append({\n",
    "        'ratio': components['ratio'].item(),\n",
    "        'clipped_ratio': components['clipped_ratio'].item(),\n",
    "        'surr1': components['surr1'].item(),\n",
    "        'surr2': components['surr2'].item(),\n",
    "        'loss': components['loss'].item()\n",
    "    })\n",
    "\n",
    "# Extract results for plotting\n",
    "ratios = [r['ratio'] for r in results]\n",
    "clipped = [r['clipped_ratio'] for r in results]\n",
    "surr1 = [r['surr1'] for r in results]\n",
    "surr2 = [r['surr2'] for r in results]\n",
    "losses = [-r['loss'] for r in results]  # Negative since we maximize\n",
    "\n",
    "# Create plot\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "# Plot 1: Ratio Clipping\n",
    "ax1.plot(ratios, ratios, label='Original ratio', color='blue')\n",
    "ax1.plot(ratios, clipped, label='Clipped ratio', color='red')\n",
    "ax1.axvline(x=0.8, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.axvline(x=1.2, color='gray', linestyle='--', alpha=0.5)\n",
    "ax1.grid(True)\n",
    "ax1.set_title('PPO Ratio Clipping')\n",
    "ax1.set_xlabel('Probability Ratio (new_prob / old_prob)')\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Surrogate Objectives and Loss\n",
    "ax2.plot(ratios, surr1, label='Surrogate 1 (unclipped)', color='blue')\n",
    "ax2.plot(ratios, surr2, label='Surrogate 2 (clipped)', color='red')\n",
    "ax2.plot(ratios, losses, label='Final objective (max)', color='green')\n",
    "ax2.grid(True)\n",
    "ax2.set_title('PPO Surrogate Objectives')\n",
    "ax2.set_xlabel('Probability Ratio (new_prob / old_prob)')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some example values\n",
    "print(\"\\nExample values for probability ratio = 1.5:\")\n",
    "idx = np.abs(np.array(ratios) - 1.5).argmin()\n",
    "print(f\"Original ratio: {ratios[idx]:.3f}\")\n",
    "print(f\"Clipped ratio: {clipped[idx]:.3f}\")\n",
    "print(f\"Surrogate 1: {surr1[idx]:.3f}\")\n",
    "print(f\"Surrogate 2: {surr2[idx]:.3f}\")\n",
    "print(f\"Final objective: {losses[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af235f1-0178-491f-8a1c-87c339a902bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b75792bd73c3afa343cab6cc62c37f65",
     "grade": true,
     "grade_id": "cell-282f8fbfa049610b",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Test 1: Basic ratio clipping\n",
    "old_probs = torch.tensor([1.0])\n",
    "new_probs = torch.tensor([1.5])\n",
    "advantages = torch.tensor([1.2])\n",
    "\n",
    "components = ppo_loss_components(old_probs, new_probs, advantages)\n",
    "assert abs(components['clipped_ratio'].item() - 1.2) < 1e-5  # Should clip at 1.2 allow for small numerical error\n",
    "assert components['surr1'].item() > components['surr2'].item() # Unclipped > clipped for ratio > 1\n",
    "assert components['loss'].item() < 0 # Loss should be negative since we maximize\n",
    "assert abs(components['loss'].item()) < components['surr1'].item() # Abs loss should be smaller than unclipped surrogate\n",
    "assert abs(components['loss'].item()) == components['surr2'].item() # Loss should be negative since we maximize\n",
    "\n",
    "# Test 2: Negative advantages\n",
    "advantages = torch.tensor([-1.0])\n",
    "components = ppo_loss_components(old_probs, new_probs, advantages)\n",
    "assert components['surr1'].item() < components['surr2'].item()  # Min should pick surr1\n",
    "\n",
    "# Test 3: Within clipping range\n",
    "new_probs = torch.tensor([1.1])\n",
    "advantages = torch.tensor([1.0])\n",
    "components = ppo_loss_components(old_probs, new_probs, advantages)\n",
    "assert torch.allclose(components['ratio'], components['clipped_ratio'])\n",
    "\n",
    "# Test 4: Batch processing\n",
    "old_probs = torch.tensor([1.0, 1.0, 1.0])\n",
    "new_probs = torch.tensor([0.5, 1.0, 2.0])\n",
    "advantages = torch.tensor([1.0, 1.0, 1.0])\n",
    "components = ppo_loss_components(old_probs, new_probs, advantages)\n",
    "assert len(components['ratio']) == 3\n",
    "\n",
    "print(\"All tests passed! ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e557f1-5702-41de-b117-fe5e08f6e726",
   "metadata": {
    "revert": ""
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
