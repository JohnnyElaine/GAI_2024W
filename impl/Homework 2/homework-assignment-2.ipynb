{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78d5361c-663a-4198-b847-d480087261f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4b2c03d3e6c50da2e3a8558dbee3683",
     "grade": false,
     "grade_id": "cell-087b71a01dc13ac4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2024.11.01 - Generative AI | Homework Assignment 2\n",
    "\n",
    "In this exercise, you will implement the food-for-thought-generating prompts strategy presented by Prof. Gottlob in his lecture. In the food-for-thought-generating prompt strategy we want, we first want to ask an LLM to provide questions that help solve the original query.\n",
    "\n",
    "\n",
    "*Relevant-Slide:*\n",
    "<img src=\"./food-for-thought.png\"/>\n",
    "\n",
    "Passages where you should add your implementation are marked with:\n",
    "\n",
    "\\# YOUR CODE HERE</br>\n",
    "raise NotImplementedError()\n",
    "\n",
    "We have provided a simple Hugging Face wrapper so you can test your implementation against an actual llm. To do so, you will need to provide your [personal access token](https://huggingface.co/docs/hub/security-tokens).\n",
    "The grading is based on a mocked client, similar to visible test cases. Therefore, when you are confident with your implementation, delete your personal access token before submitting the assignment."
   ]
  },
  {
   "cell_type": "code",
   "id": "1b588a32-7e10-4a65-b2ae-55afe423772b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7579a07d4107991067c1f4e00ce92aa8",
     "grade": false,
     "grade_id": "cell-7d0507cf18a6ba54",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:19.824601Z",
     "start_time": "2024-11-19T01:17:19.821700Z"
    }
   },
   "source": [
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import requests\n",
    "import re\n",
    "from abc import ABC, abstractmethod"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "7541db0c-f3cc-4ba7-a436-03afbf34dc1c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "480bff9c287696915e034fccdc611673",
     "grade": false,
     "grade_id": "cell-b081763e2fde4c03",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:19.851706Z",
     "start_time": "2024-11-19T01:17:19.847203Z"
    }
   },
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    role: str\n",
    "    content: str\n",
    "\n",
    "class LLMClientInterface(ABC):\n",
    "    @abstractmethod\n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        pass\n",
    "\n",
    "class HuggingFaceClient(LLMClientInterface):\n",
    "    def __init__(self, api_token: str, model_name: str = \"HuggingFaceH4/zephyr-7b-alpha\"):\n",
    "        self.api_token = api_token\n",
    "        self.model_name = model_name\n",
    "        self.api_url = f\"https://api-inference.huggingface.co/models/{model_name}/v1/chat/completions\"\n",
    "        self.headers = { \"Authorization\": f\"Bearer {api_token}\", \"Content-Type\": \"application/json\" }\n",
    "\n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        messages_dict = [\n",
    "            {\"role\": message.role, \"content\": message.content} for message in messages\n",
    "        ]\n",
    "        payload = { \"model\": self.model_name, \"messages\": messages_dict, \"max_tokens\": 500 }\n",
    "        response = requests.post(self.api_url, headers=self.headers, json=payload)\n",
    "        if response.status_code != 200:\n",
    "            raise Exception(f\"API request failed with status {response.status_code}: {response.text}\")\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]            "
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "5f7c8f56-dcf5-4f77-b797-129c81a2e6a6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20ca71cb26ccdc527ab146a77956cdbc",
     "grade": false,
     "grade_id": "cell-ccfc7bfa58208b96",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "revert": "class FoodForThoughtPrompting:\n    \"\"\"\n    Implements the food-for-thought prompting strategy for enhanced LLM interactions.\n    This strategy breaks down a complex query into sub-questions, gets their answers,\n    and synthesizes a final response using this additional context.\n    \"\"\"\n    def __init__(self, llm_client: LLMClientInterface):\n        self.llm_client = llm_client\n        self.system_prompt = \"You strive to answer the question as truthfully, precisely and concisely as possible.\"\n    \n    def generate_questions(self, query: str) -> List[str]:\n        \"\"\"\n        Generates three relevant sub-questions that help break down the main query.\n        \n        Implementation Guidelines:\n        1. Create a prompt that asks the LLM to generate 3 questions whose answers would help solve \n           the original query.\n        \n        2. Get the LLM's response using self.llm_client.get_completion(messages), where messages is a list of a user prompt and maybe a system prompt (optional).\n            e.g. messages = [\n                Message(role=\"system\", content=\"Answer truthfully and concisely [...]\"),\n                Message(role=\"user\", content=\"Which are the three questions Q1, [...]\")\n            ]\n\n        \n        4. Extract the three questions from the response (e.g. using regex or simple string splitting).\n           You might need to modify your user prompt if you don't get consistently formatted completions.\n        \n        Args:\n            query (str): The original query to be broken down\n            \n        Returns:\n            List[str]: List of exactly three questions\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def get_answers(self, questions: List[str]) -> List[str]:\n        \"\"\"\n        Generate answers for each of the generated sub-questions.\n\n        Args:\n            questions (List[str]): List of questions to be answered\n            \n        Returns:\n            List[str]: List of answers corresponding to each question generated by the LLM.\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def get_final_answer(self, query: str, qa_pairs: List[Tuple[str, str]]) -> str:\n        \"\"\"\n        Synthesizes a final answer using the original query and Q&A context.\n        \n        Implementation Guidelines:\n        1. Create a messages list optionally starting with the system prompt Message.\n           It might help to tell the LLM that the whole conversation should be incorporated into its final response\n           in the system prompt.\n        \n        2. For each (question, answer) pair in qa_pairs:\n           - Add two Message objects:\n             * First with role=\"user\" containing the question\n             * Second with role=\"assistant\" containing the answer\n        \n        3. Add a final Message with role=\"user\" containing the original query\n        \n        4. Get and return the final response using self.llm_client.get_completion()\n        \n        Args:\n            query (str): The original query\n            qa_pairs (List[Tuple[str, str]]): List of (question, answer) tuples providing context\n            \n        Returns:\n            str: Synthesized final answer incorporating the context\n        \"\"\"\n        # YOUR CODE HERE\n        raise NotImplementedError()\n    \n    def __call__(self, query: str) -> str:\n        try:\n            questions = self.generate_questions(query)            \n            answers = self.get_answers(questions)            \n            qa_pairs = list(zip(questions, answers))            \n            final_answer = self.get_final_answer(query, qa_pairs)\n            return final_answer\n        except Exception as e:\n            return f\"Error processing query: {str(e)}\"",
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:19.874739Z",
     "start_time": "2024-11-19T01:17:19.869221Z"
    }
   },
   "source": [
    "class FoodForThoughtPrompting:\n",
    "    \"\"\"\n",
    "    Implements the food-for-thought prompting strategy for enhanced LLM interactions.\n",
    "    This strategy breaks down a complex query into sub-questions, gets their answers,\n",
    "    and synthesizes a final response using this additional context.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm_client: LLMClientInterface):\n",
    "        self.llm_client = llm_client\n",
    "        self.system_prompt = \"You strive to answer the question as truthfully, precisely and concisely as possible.\"\n",
    "    \n",
    "    def generate_questions(self, query: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generates three relevant sub-questions that help break down the main query.\n",
    "        \n",
    "        Implementation Guidelines:\n",
    "        1. Create a prompt that asks the LLM to generate 3 questions whose answers would help solve \n",
    "           the original query.\n",
    "        \n",
    "        2. Get the LLM's response using self.llm_client.get_completion(messages), where messages is a list of a user prompt and maybe a system prompt (optional).\n",
    "            e.g. messages = [\n",
    "                Message(role=\"system\", content=\"Answer truthfully and concisely [...]\"),\n",
    "                Message(role=\"user\", content=\"Which are the three questions Q1, [...]\")\n",
    "            ]\n",
    "\n",
    "        \n",
    "        4. Extract the three questions from the response (e.g. using regex or simple string splitting).\n",
    "           You might need to modify your user prompt if you don't get consistently formatted completions.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The original query to be broken down\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of exactly three questions\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # Prepare the input messages for the LLM\n",
    "        messages = [\n",
    "            Message(role=\"system\", content=self.system_prompt + \"Generate at least three questions.\"),\n",
    "            Message(role=\"user\", content=query),\n",
    "        ]\n",
    "        \n",
    "        # Get the completion from the LLM\n",
    "        response = self.llm_client.get_completion(messages)\n",
    "        \n",
    "        # Split the response into individual questions\n",
    "        questions = [line.strip() for line in response.split(\"\\n\") if line.strip()]\n",
    "        \n",
    "        return questions[:3]\n",
    "    \n",
    "    def get_answers(self, questions: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate answers for each of the generated sub-questions.\n",
    "\n",
    "        Args:\n",
    "            questions (List[str]): List of questions to be answered\n",
    "            \n",
    "        Returns:\n",
    "            List[str]: List of answers corresponding to each question generated by the LLM.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        answers = []\n",
    "        for question in questions:\n",
    "            # Prepare the input messages for each question\n",
    "            messages = [\n",
    "                Message(role=\"system\", content=self.system_prompt),\n",
    "                Message(role=\"user\", content=question),\n",
    "            ]\n",
    "            \n",
    "            # Get the answer from the LLM\n",
    "            response = self.llm_client.get_completion(messages)\n",
    "            answers.append(response)\n",
    "            \n",
    "        return answers\n",
    "    \n",
    "    def get_final_answer(self, query: str, qa_pairs: List[Tuple[str, str]]) -> str:\n",
    "        \"\"\"\n",
    "        Synthesizes a final answer using the original query and Q&A context.\n",
    "        \n",
    "        Implementation Guidelines:\n",
    "        1. Create a messages list optionally starting with the system prompt Message.\n",
    "           It might help to tell the LLM that the whole conversation should be incorporated into its final response\n",
    "           in the system prompt.\n",
    "        \n",
    "        2. For each (question, answer) pair in qa_pairs:\n",
    "           - Add two Message objects:\n",
    "             * First with role=\"user\" containing the question\n",
    "             * Second with role=\"assistant\" containing the answer\n",
    "        \n",
    "        3. Add a final Message with role=\"user\" containing the original query\n",
    "        \n",
    "        4. Get and return the final response using self.llm_client.get_completion()\n",
    "        \n",
    "        Args:\n",
    "            query (str): The original query\n",
    "            qa_pairs (List[Tuple[str, str]]): List of (question, answer) tuples providing context\n",
    "            \n",
    "        Returns:\n",
    "            str: Synthesized final answer incorporating the context\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        messages = [\n",
    "            Message(role=\"system\", content=self.system_prompt + 'Make sure to incorporate the entire conversation into your final response'),\n",
    "        ]\n",
    "        \n",
    "        for i in range(len(qa_pairs)):\n",
    "            question, answer = qa_pairs[i]\n",
    "            messages.append(Message(role=\"user\", content=question))\n",
    "            messages.append(Message(role=\"assistant\", content=answer))\n",
    "            \n",
    "        messages.append(Message(role=\"user\", content=query))\n",
    "        \n",
    "        # Get the final answer from the LLM\n",
    "        return self.llm_client.get_completion(messages)\n",
    "    \n",
    "    def __call__(self, query: str) -> str:\n",
    "        try:\n",
    "            questions = self.generate_questions(query)            \n",
    "            answers = self.get_answers(questions)            \n",
    "            qa_pairs = list(zip(questions, answers))            \n",
    "            final_answer = self.get_final_answer(query, qa_pairs)\n",
    "            return final_answer\n",
    "        except Exception as e:\n",
    "            return f\"Error processing query: {str(e)}\""
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "b5d4c7ad-cd40-4b26-aea1-820b7192f52b",
   "metadata": {
    "revert": "# To test your implementation against a real LLM, provide your hugging face access token below.\n# Before submitting it, please remove your token again. Grading will be based on a mocked implementation\n# If the response seems cut off - no worries; its a known issue\n# https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/52\nhuggingface_client = HuggingFaceClient(\"YOUR_HUGGING_FACE_TOKEN_HERE\")\nfft_prompting = FoodForThoughtPrompting(huggingface_client)\nfft_prompting(\"How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\")",
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:21.827655Z",
     "start_time": "2024-11-19T01:17:19.875744Z"
    }
   },
   "source": [
    "# To test your implementation against a real LLM, provide your hugging face access token below.\n",
    "# Before submitting it, please remove your token again. Grading will be based on a mocked implementation\n",
    "# If the response seems cut off - no worries; its a known issue\n",
    "# https://huggingface.co/HuggingFaceH4/zephyr-7b-beta/discussions/52\n",
    "huggingface_client = HuggingFaceClient(\"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\")\n",
    "fft_prompting = FoodForThoughtPrompting(huggingface_client)\n",
    "fft_prompting(\"How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_questions: How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\n",
      "get_answers questions: ['1. On a scale of 1 to 10, how similar are a pen and a marker in terms of their materials and purpose?', '2. Which items would you consider more similar between a pen and a marker, based on their functionalities or their form: pen nibs or marker tips?', '3. What specific differences distinguish pen usage from marker usage, and how significantly do these differences impact the overall usability of each tool for various tasks?']\n",
      "get_final_answer: How similar are a pen and a marker, on a score ranging from 1 (lowest) to 10 (highest)?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Based solely on materials and purpose, pen and marker are similar to some extent. They both dispense ink onto a surface, but their intended purposes and applications are different, which ultimately impact their usability for various tasks. \\n\\nOn a scale of 1 to 10, the similarity rating between pen and marker would be 4. While they have similarities in terms of materials, their functionalities and intended purposes differ significantly. Pens are generally used for finer lines, and can be used for a variety of writing applications, while markers are typically used for coloring, painting, or highlighting activity.\\n\\nOverall, while there are some similarities between pens and markers, their differences significantly impact their usability for various tasks, and ultimately affect how we perceive and use them.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "5d41676f-9ad7-4fab-85b7-a30a9fae75fe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8c4c08b744df2dec345b041c5b5fb20",
     "grade": true,
     "grade_id": "cell-e7117f7da4e0dd04",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:21.833436Z",
     "start_time": "2024-11-19T01:17:21.827655Z"
    }
   },
   "source": [
    "class MockLLMClient(LLMClientInterface):\n",
    "    def __init__(self, responses):\n",
    "        self.responses = responses\n",
    "        self.call_count = 0\n",
    "        self.messages_history = []\n",
    "    \n",
    "    def get_completion(self, messages: List[Message]) -> str:\n",
    "        self.messages_history.append(messages)\n",
    "        response = self.responses[self.call_count]\n",
    "        self.call_count += 1\n",
    "        return response\n",
    "\n",
    "def test_generate_questions_success():\n",
    "    mock_response = \"\"\"Q1: What are your dietary restrictions or preferences?\n",
    "Q2: What time of day will you be eating lunch?\n",
    "Q3: What is your budget for lunch?\"\"\"\n",
    "    \n",
    "    client = MockLLMClient([mock_response])\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    questions = fft.generate_questions(\"What is the best lunch to eat?\")\n",
    "    \n",
    "    assert len(questions) == 3\n",
    "    assert all(isinstance(q, str) for q in questions)\n",
    "    assert \"dietary restrictions\" in questions[0]\n",
    "    assert \"time of day\" in questions[1]\n",
    "    assert \"budget\" in questions[2]\n",
    "\n",
    "def test_get_answers():\n",
    "    mock_responses = [\n",
    "        \"You should consider a balanced meal with protein and vegetables.\",\n",
    "        \"Lunch is typically eaten between 12:00 and 2:00 PM.\",\n",
    "        \"A reasonable budget is $10-15 for a healthy lunch.\"\n",
    "    ]\n",
    "    \n",
    "    client = MockLLMClient(mock_responses)\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    questions = [\n",
    "        \"What makes a healthy lunch?\",\n",
    "        \"When should I eat lunch?\",\n",
    "        \"How much should I spend on lunch?\"\n",
    "    ]\n",
    "    \n",
    "    answers = fft.get_answers(questions)\n",
    "    assert len(answers) == 3\n",
    "    assert all(isinstance(a, str) for a in answers)\n",
    "    assert \"balanced meal\" in answers[0]\n",
    "    assert \"12:00\" in answers[1]\n",
    "    assert \"$10-15\" in answers[2]\n",
    "\n",
    "def test_process_query_end_to_end():\n",
    "    mock_responses = [\n",
    "        # Questions generation response\n",
    "        \"\"\"Q1: What are your dietary restrictions?\n",
    "Q2: What time of day will you eat?\n",
    "Q3: What is your budget?\"\"\",\n",
    "        # Three answers\n",
    "        \"No dietary restrictions.\",\n",
    "        \"Lunchtime at 12:30 PM.\",\n",
    "        \"Budget is $15.\",\n",
    "        # Final answer\n",
    "        \"Based on the information provided, I recommend a balanced meal...\"\n",
    "    ]\n",
    "    \n",
    "    client = MockLLMClient(mock_responses)\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    result = fft(\"What should I eat for lunch?\")\n",
    "    assert isinstance(result, str)\n",
    "    assert \"balanced meal\" in result\n",
    "    assert client.call_count == 5  # 1 for questions + 3 for answers + 1 for final\n",
    "\n",
    "def test_error_handling():\n",
    "    client = MockLLMClient([])  # Empty responses will cause index error\n",
    "    fft = FoodForThoughtPrompting(client)\n",
    "    \n",
    "    result = fft(\"What should I eat?\")\n",
    "    assert \"Error processing query\" in result\n",
    "\n",
    "\n",
    "test_generate_questions_success()\n",
    "test_get_answers()\n",
    "test_process_query_end_to_end()\n",
    "test_error_handling()\n",
    "\n",
    "print(\"If you see this message, you are good to go ✅\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate_questions: What is the best lunch to eat?\n",
      "get_answers questions: ['What makes a healthy lunch?', 'When should I eat lunch?', 'How much should I spend on lunch?']\n",
      "generate_questions: What should I eat for lunch?\n",
      "get_answers questions: ['Q1: What are your dietary restrictions?', 'Q2: What time of day will you eat?', 'Q3: What is your budget?']\n",
      "get_final_answer: What should I eat for lunch?\n",
      "generate_questions: What should I eat?\n",
      "If you see this message, you are good to go ✅\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T01:17:21.835464Z",
     "start_time": "2024-11-19T01:17:21.833436Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2ad8a42f7395082",
   "outputs": [],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
