{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2d3a18-bfd2-4a7c-b98d-197ea4fd09de",
   "metadata": {},
   "source": [
    "### 2023.11.23 - Introduction to Transformers Continued |Â Homework 3\n",
    "Proximal Policy Optimization (PPO) is a type of policy gradient method for reinforcement learning that was used in the final training stage of GPT. In this homework, you will implement a highly simplified version of the PPO algorithm to get a foundational understanding of its mechanics. The aim is to outline the whole concept with mostly static components, to give you a birds eye view of how it works.\n",
    "\n",
    "The endresult is a model that learned that the reward model prefers positive words over negative ones.\n",
    "\n",
    "We know that there is quite a long path between this excercise and the real world application of PPO. However, since this is an introductory lecture and due to the vast diversity of the audience we will noo go into more depth.\n",
    "In case you are interested in diving deeper into the realm of reinforcement learning and PPO, checkout the following resources:\n",
    "- [OpenAI Spinning Up](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)\n",
    "- [Lilian Weng Policy Gradient Algorithms](https://lilianweng.github.io/posts/2018-04-08-policy-gradient/)\n",
    "- [Proximal Policy Optimization (PPO): The Key to LLM Alignment](https://cameronrwolfe.substack.com/p/proximal-policy-optimization-ppo)\n",
    "\n",
    "Base your code on the following skeleton code that we provide.\n",
    "\n",
    "We tried to explain our simplifications as much as possible, to help you on the one hand understand the high level concept and on the other are able to connect it to real world implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05375d1-5a67-4446-9856-0a6feb756c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import medfilt\n",
    "# Dependencies you might need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22cd48e-3140-43d9-85fc-183b442a3396",
   "metadata": {},
   "source": [
    "##### Agent and Policy:\n",
    "In RLHF for LLMs, the agent is the pre-trained language model itself. The policy is represented by the model's forward function. In our case instead of using a full language model we will use a simple Multilayer perceptron (MLP) that in the end should predict positive words more likeyl than negative words, independently of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0131a6-4c2f-4c4c-af09-aff9c153fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of words and their corresponding \"positivity\" score\n",
    "word_scores = {\"happy\": 1, \"joyful\": 1, \"pleasant\": 1, \"sad\": -1, \"angry\": -1, \"unpleasant\": -1}\n",
    "words = list(word_scores.keys())\n",
    "positive_words = set({k:v for k,v in word_scores.items() if v == 1}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc48ad0b-fdc3-4f93-8d58-ac9847446582",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    \"\"\"\n",
    "    The Agent class represents a simplified version of a policy model used in Proximal Policy Optimization (PPO) that operates as\n",
    "    a basic language model. The model is a Multilayer Perceptron (MLP) designed to predict the likelihood of positive words\n",
    "    over negative ones, regardless of the input.\n",
    "\n",
    "    Example:\n",
    "        # Example instantiation and forward pass\n",
    "        agent = Agent(num_words)\n",
    "        input_data = torch.tensor([...])  # Some one-hot encoded input data\n",
    "        output = agent(input_data)\n",
    "    \"\"\"\n",
    "    # A simple MLP with 2 linear layers and a ReLU activation in between\n",
    "    def __init__(self, num_words):\n",
    "        super(Agent, self).__init__()\n",
    "        \"\"\"\n",
    "        TODO: Define the architecture of the MLP: Decide on the number of layers, units in each layer, and activation functions.\n",
    "        Hint: You can start as simple as 2 linear layers of low dimension (e.g. 128) with a simple ReLU activation function in between. (nn.Linear, torch.relu)\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Implement the forward pass: Define how the input data flows through the network to produce output.\n",
    "        The output should be a probability distribution over words. (Hint: you might want to use torch.softmax for that)\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3af36fb-6699-4c2f-b9d5-02c63f977f8e",
   "metadata": {},
   "source": [
    "##### Value Function/Critic/Reward Model\n",
    "The critic is a separate model or mechanism that evaluates the quality of the outputs generated by the agent.\n",
    "In the context of RLHF, this could be a model trained to predict human preferences or judgments regarding the quality or appropriateness of the model's responses.\n",
    "\n",
    "In our case we just return the predefined positivy score for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc300bcc-c4fe-4c8c-9af1-6d9e91ffe876",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic:\n",
    "    \"\"\"\n",
    "    A simple static model that simply uses our word_score lookup table to \"judge\" a given input.\n",
    "    In real scenarios, critics can be complex models trained to evaluate text based on various criteria like coherence, grammar,\n",
    "    and alignment with human preferences.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "    def forward(self, word):\n",
    "        return torch.tensor([word_scores.get(word, 0)])\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.forward(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcfe9bd-ddb3-44e0-b31a-c89e72786985",
   "metadata": {},
   "source": [
    "##### Baseline\n",
    "\n",
    "The primary purpose of using a baseline is to reduce the variance of the gradient estimate without biasing it. In policy gradient methods, the goal is to maximize the expected reward. However, the gradient of this expectation can have high variance, leading to inefficient learning. By subtracting a baseline from the reward, the variance of the gradient can be significantly reduced, which typically results in more stable and efficient learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34085d5f-24e1-4897-baea-3a5ef6067bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline:\n",
    "    \"\"\"\n",
    "    The Baseline class in this exercise represents a mechanism to calculate and update a baseline value used \n",
    "    in the Proximal Policy Optimization (PPO) algorithm. The baseline is utilized to reduce the variance in the\n",
    "    policy gradient estimation, leading to more stable and efficient learning.\n",
    "\n",
    "    In this exercise, you will implement and use the Baseline class to compute an average score.\n",
    "    \n",
    "    Example:\n",
    "        # Example of updating and getting the baseline\n",
    "        baseline = Baseline()\n",
    "        baseline.update(new_score=1.0)\n",
    "        current_baseline = baseline.get()\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.total_score = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, new_score):\n",
    "        # TODO: Implement the update method: Update the total score and count based on the new score received after each action.\n",
    "        pass\n",
    "\n",
    "    def get(self):\n",
    "        # TODO: Implement the get method: Calculate and return the current average score (baseline).\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb095d7-3687-423a-b6cb-ed8433324896",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dedb0d-4453-4e59-9a5a-af0c152c104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(agent, critic, optim, epsilon=0.01, iter=100):\n",
    "    \"\"\"\n",
    "    This function represents the training loop where the Proximal Policy Optimization (PPO) algorithm is applied\n",
    "    to train the Agent. The function takes the agent, critic, and optimizer as inputs and performs iterative training.\n",
    "\n",
    "    The loop involves selecting actions (words), evaluating them, and updating the agent's policy based on the calculated loss.\n",
    "\n",
    "\n",
    "    Parameters:\n",
    "        agent (Agent): The policy model (MLP) being trained.\n",
    "        critic (Critic): The model used to evaluate the quality of actions.\n",
    "        optimizer (torch.optim): The optimizer used for updating the agent.\n",
    "        epsilon (float): The probability of choosing a random action (exploration).\n",
    "        iter (int): The number of training iterations.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the loss and positive word count metrics over training iterations.\n",
    "\n",
    "    Example:\n",
    "        # Example training call\n",
    "        agent = Agent(len(words))\n",
    "        critic = Critic()\n",
    "        optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "        losses, positive_word_counts = train(agent, critic, optimizer)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Training Metrics\n",
    "    losses = []\n",
    "    positive_word_counts = []\n",
    "    total_positive_words = 0\n",
    "\n",
    "    onehot_encorder = torch.eye(len(words))\n",
    "    \n",
    "    for i in range(iter):\n",
    "        # In our case our \"environment\" is just the random selection of a word\n",
    "        random_idx = random.randint(0, len(words) - 1)\n",
    "        input = onehot_encorder[random_idx]\n",
    "        \n",
    "        agent_output = agent(input)        \n",
    "\n",
    "        # TODO: Implement the logic for choosing actions based on exploration or exploitation.\n",
    "        # Hint: You might want to decide between choosing a random word_index as a suggestions vs using something like torch.agrmax\n",
    "        # on the agents output. (e.g. if random.random() < epsilon: choose random word else choose based on probability distribution\n",
    "        # Important this section should assign a value to the var \"word_index\"\n",
    "\n",
    "        chosen_word = words[word_index] # chosen word (=the suggested action)\n",
    "\n",
    "        # TODO: calculate the critic_score score\n",
    "        \n",
    "        # TODO: update the baseline using the critic_score     \n",
    "        \n",
    "        # TODO: calculate the advantage as the difference between the critic_score and the baseline\n",
    "        \n",
    "        # TODO: Calculate loss as the negative log of the probability of the chosen_word multiplied by the advantage\n",
    "        \n",
    "        # Update the agent\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Record and compute moving average of the loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        if chosen_word in positive_words:\n",
    "            total_positive_words += 1\n",
    "        positive_word_counts.append(total_positive_words / (i + 1))\n",
    "    \n",
    "        if i % 1 == 0:\n",
    "            print(f\"Iteration {i}, Loss: {loss.item()}, Generated Word: {chosen_word}\")\n",
    "\n",
    "    return losses, positive_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537cf333-80f3-4dd3-bc5a-30af6d8cf5c8",
   "metadata": {},
   "source": [
    "#### Run Exercise 1\n",
    "Run this cell to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e8d64e-a98a-4758-a24e-57dce1f3f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Expected Outcome:\n",
    "Iteration 0, Loss: 0.0, Generated Word: angry\n",
    "Iteration 1, Loss: 2.910965919494629, Generated Word: pleasant\n",
    "...\n",
    "Note: The losses as well as the generated word will obviously be different.\n",
    "However you should see that the number of positive words is increasing over time.\n",
    "\n",
    "!! Important !! : With low model complexity and random weights initalizaion the model might get stuck\n",
    "and produce a constant result with 0 loss. Just rerun this cell.\n",
    "\"\"\"\n",
    "agent = Agent(len(words))\n",
    "critic = Critic()\n",
    "\n",
    "baseline = Baseline()\n",
    "optimizer = optim.Adam(agent.parameters(), lr=3e-4)\n",
    "\n",
    "losses, positive_word_counts = train(agent, critic, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c0011b-a322-41f4-8f14-a1c380c8a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Expected Outcome:\n",
    "You should see 3 figures, the first two should show a declining slop, the third one should look more like a log function.\n",
    "\"\"\"\n",
    "filtered_loss = medfilt(losses, kernel_size=5)\n",
    "\n",
    "# Plot the filtered loss\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(losses)\n",
    "plt.title(\"Raw Loss over Time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Filtered Loss\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(filtered_loss)\n",
    "plt.title(\"Loss over Time\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Filtered Loss\")\n",
    "\n",
    "# Plotting the frequency of choosing positive words\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(positive_word_counts)\n",
    "plt.title(\"Frequency of Choosing Positive Words\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
