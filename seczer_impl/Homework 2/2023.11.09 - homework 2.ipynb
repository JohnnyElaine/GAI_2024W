{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd2d3a18-bfd2-4a7c-b98d-197ea4fd09de",
   "metadata": {},
   "source": [
    "### 2023.11.09 - Introduction to Transformers Continued |Â Homework 2\n",
    "In this exercise, you will implement a masked scaled dot-product attention, which allows the model to focus on different parts of the input sequence.\n",
    "\n",
    "Base your code on the following skeleton code that we provide:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176b9091-10ea-428f-92e0-147d832beef8",
   "metadata": {},
   "source": [
    "### Exercise 1 - Scaled Dot-Product Attention:\n",
    "Implement the scaled dot-product attention function, which is defined as `Attention(Q, K, V) = softmax(QK^T / sqrt(d_k))V`, where Q, K, V are queries, keys, and values, respectively, and `d_k` is the dimensionality of the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af6d0e24-537c-4c11-bb0b-36e89d209295",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "# TODO: Import necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ae1d01c0-25ac-457a-89cf-c6eb63e9bef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor) -> (Tensor, Tensor):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "    The function takes three inputs: Q (query), K (key), V (value).\n",
    "    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - K: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - V: Tensor of shape (batch_size, block_size, d_k)\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (batch_size, block_size, d_k)\n",
    "        The resulting tensor after applying the attention mechanism.\n",
    "    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n",
    "        The attention weights after applying softmax.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2881b-809c-4725-8c38-0e7280489312",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Run Exercise 1\n",
    "Run this cell to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28389ba2-4192-413d-a14a-e2e21e4e49a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Scaled dot-product attention\n",
      "Attention Output:\n",
      "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
      "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
      "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
      "\n",
      "        [[-0.1859, -0.8284, -0.5916, -1.0532],\n",
      "         [-0.1266, -0.8076, -0.5838, -1.1651],\n",
      "         [ 0.1222, -0.4625, -0.4709, -0.1724]]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[[0.4537, 0.3877, 0.1586],\n",
      "         [0.4461, 0.3433, 0.2105],\n",
      "         [0.2678, 0.3638, 0.3684]],\n",
      "\n",
      "        [[0.5597, 0.1483, 0.2920],\n",
      "         [0.5515, 0.0702, 0.3784],\n",
      "         [0.1524, 0.7299, 0.1178]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "============ Scaled dot-product attention\n",
    "Attention Output:\n",
    "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081], # these numbers will be different, but the dimensions should be the same\n",
    "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
    "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
    "\n",
    "        [[-0.1859, -0.8284, -0.5916, -1.0532],\n",
    "         [-0.1266, -0.8076, -0.5838, -1.1651],\n",
    "         [ 0.1222, -0.4625, -0.4709, -0.1724]]])\n",
    "\n",
    "Attention Weights:\n",
    "tensor([[[0.4537, 0.3877, 0.1586],\n",
    "         [0.4461, 0.3433, 0.2105],\n",
    "         [0.2678, 0.3638, 0.3684]],\n",
    "\n",
    "        [[0.5597, 0.1483, 0.2920],\n",
    "         [0.5515, 0.0702, 0.3784],\n",
    "         [0.1524, 0.7299, 0.1178]]])\n",
    "\"\"\"\n",
    "print(\"============ Scaled dot-product attention\")\n",
    "torch.manual_seed(0)      # to keep results consistent between runs\n",
    "Q = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "K = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "V = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V)\n",
    "print(f\"Attention Output:\\n{output}\\n\")\n",
    "print(f\"Attention Weights:\\n{weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb52802-4d2f-4215-a3c2-e1911f371a99",
   "metadata": {},
   "source": [
    "### Excercise 2 -  Add a mask to the attention mechanism:\n",
    "The next step is to add a masking functionality to the attention mechanism.\n",
    "A mask allows the model to selectively ignore certain positions within the sequence, used to handle variable sequence lengths or to prevent the model from attending to future positions in the a decoder model.\n",
    "\n",
    "\n",
    "_Note:_\n",
    "The mask should have the same batch size and block size as the queries and should contain ones for positions that should be attended to and zeros for masked positions. Before the softmax step, masked positions are filled with large negative values (e.g. -torch.inf), effectively removing these positions from consideration in the attention weights.\n",
    "\n",
    "Feel free to copy & paste your implementation from Excercise 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9ce8dc52-5b66-43ad-9771-d19a5fc17412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, mask: Tensor=None) -> (Tensor, Tensor):\n",
    "    \"\"\"\n",
    "    Computes the scaled dot-product attention.\n",
    "    The function takes three inputs: Q (query), K (key), V (value).\n",
    "    It computes the dot products of the query with all keys, divides each by sqrt(d_k), and applies a softmax function to obtain the weights on the values.\n",
    "\n",
    "    Parameters:\n",
    "    - Q: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - K: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - V: Tensor of shape (batch_size, block_size, d_k)\n",
    "    - mask: Optional tensor of shape (batch_size, 1, block_size).\n",
    "            The mask should contain 1s for positions to attend to and 0s for masked positions.\n",
    "            Masked positions are filled with large negative values before the softmax step, effectively excluding them from consideration in the attention weights.\n",
    "\n",
    "    Returns:\n",
    "    - output: Tensor of shape (batch_size, block_size, d_k)\n",
    "        The resulting tensor after applying the attention mechanism.\n",
    "    - attention_weights: Tensor of shape (batch_size, block_size, block_size)\n",
    "        The attention weights after applying softmax.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3601c8-5d07-4d72-b57d-5c4b55203411",
   "metadata": {},
   "source": [
    "#### Run Exercise 2\n",
    "Run these cells to evaluate your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "019463a3-6e83-4737-8111-ce439c842afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Masked scaled dot-product attention\n",
      "Attention Output:\n",
      "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
      "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
      "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
      "\n",
      "        [[-0.5912, -1.0937, -0.6829, -0.9884],\n",
      "         [-0.6886, -1.1866, -0.7139, -1.1376],\n",
      "         [ 0.0322, -0.4995, -0.4843, -0.0339]]])\n",
      "\n",
      "Attention Weights:\n",
      "tensor([[[0.4537, 0.3877, 0.1586],\n",
      "         [0.4461, 0.3433, 0.2105],\n",
      "         [0.2678, 0.3638, 0.3684]],\n",
      "\n",
      "        [[0.7905, 0.2095, 0.0000],\n",
      "         [0.8871, 0.1129, 0.0000],\n",
      "         [0.1727, 0.8273, 0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Expected Output:\n",
    "============ Masked scaled dot-product attention\n",
    "Attention Output:\n",
    "tensor([[[ 0.2957,  0.3332,  0.0674,  0.2081],\n",
    "         [ 0.2628,  0.4036,  0.0423,  0.2148],\n",
    "         [ 0.3083,  0.4486,  0.2576,  0.2905]],\n",
    "\n",
    "        [[-0.5912, -1.0937, -0.6829, -0.9884],\n",
    "         [-0.6886, -1.1866, -0.7139, -1.1376],\n",
    "         [ 0.0322, -0.4995, -0.4843, -0.0339]]])\n",
    "\n",
    "Attention Weights:\n",
    "tensor([[[0.4537, 0.3877, 0.1586],\n",
    "         [0.4461, 0.3433, 0.2105],\n",
    "         [0.2678, 0.3638, 0.3684]],\n",
    "\n",
    "        [[0.7905, 0.2095, 0.0000],\n",
    "         [0.8871, 0.1129, 0.0000],\n",
    "         [0.1727, 0.8273, 0.0000]]])\n",
    "\"\"\"\n",
    "print(\"============ Masked scaled dot-product attention\")\n",
    "torch.manual_seed(0)      # to keep results consistent between runs\n",
    "Q = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "K = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "V = torch.randn(2, 3, 4)  # batch size of 2, block size of 3, and d_k of 4\n",
    "\n",
    "mask = torch.ones_like(Q[:, :, 0]).unsqueeze(1)  # Mask of shape (batch size, 1, block size)\n",
    "mask[1, 0, 2:] = 0 # Mask last token of the second block\n",
    "\n",
    "output, weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "print(f\"Attention Output:\\n{output}\\n\")\n",
    "print(f\"Attention Weights:\\n{weights}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
